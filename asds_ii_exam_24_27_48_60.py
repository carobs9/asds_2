# -*- coding: utf-8 -*-
"""ASDS_II_Exam_24_27_48_60

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1C6EiBxkoedjTFRv_ZJU8SGR840g4nl1_

<h1><center>ASDS II</center></h1>
<h2><center>Social Data Science 2023 - Exam</center></h1>
<h3><center>Exam numbers:  24 - 27 - 48  - 60 </center></h3>

All answers are written jointly by the group, and assigned exam numbers are arbitrary

# Table of Contents

<h2>&emsp;<a href="#1">Part 1: Measuring grandstanding in parliaments</a></h2>
<blockquote>
    <h3>&emsp;&emsp;<a href="#1.1">1.1. Justification (27)</a></h3>
</blockquote>
<blockquote>
    <h3>&emsp;&emsp;<a href="#1.2">1.2. ANEW Dictionary Augmentation (60)</a></h3>
</blockquote>
<blockquote>
    <h3>&emsp;&emsp;<a href="#1.3">1.3. Alternative Method (48)</a></h3>
</blockquote>

<h2>&emsp;<a href="#2">Part 2: Detecting Harassment </a></h2>
<blockquote>
    <h3>&emsp;&emsp;<a href="#2.1">2.1. Dictionary Method (24) </a></h3>
</blockquote>
<blockquote>
    <h3>&emsp;&emsp;<a href="#2.2">2.2. Topic Model (24) </a></h3>
</blockquote>
<blockquote>
    <h3>&emsp;&emsp;<a href="#2.3">2.3. Supervised Learning (60) </a></h3>
</blockquote>

<h2>&emsp;<a href="#3">Part 3: Supervised Text Classification</a></h2>
<blockquote>
   <h3>&emsp;&emsp;<a href="#3.0">3.0. Packages, data, functions and data transformation</a></h3>
<blockquote>
    <h4>&emsp;&emsp;<a href="#3.0.1.">3.0.1.Packages (24, 27 48, 60) </a></h4>
</blockquote>

<blockquote>
    <h4>&emsp;&emsp;<a href="#3.0.2.">3.0.2.Data (24, 27 48, 60)</a></h4>
</blockquote>

<blockquote>
    <h4>&emsp;&emsp;<a href="#3.0.3.">3.0.3.Functions (24, 27 48, 60)</a></h4>
</blockquote>
<blockquote>
    <h4>&emsp;&emsp;<a href="#3.0.4.">3.0.4.Transforms Data (24, 27 48, 60)</a></h4>
</blockquote>
</blockquote>
<blockquote>
    <h3>&emsp;&emsp;<a href="#3.1">3.1.BoW Models (48) </a></h3>


<blockquote>
    <h4>&emsp;&emsp;<a href="#3.1.1.">3.1.1.Logistic Regression on Unprocessed Data</a></h4>
</blockquote>

<blockquote>
    <h4>&emsp;&emsp;<a href="#3.1.2.">3.1.2.Logistic Regression on Preprocessed Data</a></h4>
</blockquote>

<blockquote>
    <h4>&emsp;&emsp;<a href="#3.1.3.">3.1.3.SVC on Unprocessed Data</a></h4>
</blockquote>

<blockquote>
    <h4>&emsp;&emsp;<a href="#3.1.4.">3.1.4.SVC on Preprocessed Data</a></h4>
</blockquote>

<blockquote>
    <h4>&emsp;&emsp;<a href="#3.1.5.">3.1.5.Multinomial Naive Bayes on Unprocessed Data</a></h4>
</blockquote>

<blockquote>
    <h4>&emsp;&emsp;<a href="#3.1.6.">3.1.6.Multinomial Naive Bayes on Preprocessed Data</a></h4>
</blockquote>
</blockquote>

<blockquote>
    <h3>&emsp;&emsp;<a href="#3.2">3.2.Deep Learning</a></h3>
<blockquote>
    <h4>&emsp;&emsp;<a href="#3.2.1.">3.2.1.RNNs-LSTM Models (27)</a></h4>
</blockquote>

<blockquote>
    <h4>&emsp;&emsp;<a href="#3.2.2.">3.2.2.BERT fine-tuned (27)</a></h4>
</blockquote>
</blockquote>
<blockquote>
    <h3>&emsp;&emsp;<a href="#3.3">3.3.Models Discussion (60) </a></h3>
<blockquote>
    <h4>&emsp;&emsp;<a href="#3.3.1.">3.3.1.BoW</a></h4>
</blockquote>
<blockquote>
    <h4>&emsp;&emsp;<a href="#3.3.2.">3.3.2.Deep Learning (RNN-LSTM)</a></h4>
</blockquote>
<blockquote>
    <h4>&emsp;&emsp;<a href="#3.3.3.">3.3.3.Deep Learning (BERT)</a></h4>
</blockquote>
</blockquote>
<blockquote>
    <h3>&emsp;&emsp;<a href="#3.4">3.4.Performance (27)</a></h3>
</blockquote>
<blockquote>
    <h3>&emsp;&emsp;<a href="#3.5">3.5.Discussion (48) </a></h3>
</blockquote>

<h2>&emsp;<a href="#4">Part 4: Word Embeddings</a></h2>
<blockquote>
    <h3>&emsp;&emsp;<a href="#4.1">4.1.Training (24)</a></h3>
</blockquote>
<blockquote>
    <h3>&emsp;&emsp;<a href="#4.2">4.2.Similarity (24)</a></h3>
</blockquote>
<blockquote>
    <h3>&emsp;&emsp;<a href="#4.3">4.3.Supervised Task (24)</a></h3>
</blockquote>
<blockquote>
    <h3>&emsp;&emsp;<a href="#4.4">4.4.Re-training (24)</a></h3>
</blockquote>
<blockquote>
    <h3>&emsp;&emsp;<a href="#4.5">4.5.Validation (60)</a></h3>
</blockquote>
<blockquote>

<h2>&emsp;<a href="#References">References</a></h2>
<blockquote>

<a name="1"></a>

# Part 1: Measuring Grandstanding in Parliaments

<a name="1.1"></a>

## 1.1. Justification

In regard to their research, the authors argue it was better to apply a dictionary-based approach instead of training a supervised learning model, for which they would have needed to manually annotate a subset of the speeches.

First, using a dictionary-based approach allows the authors to score texts along the specific dimension of emotions (Hovy 1, p.38). Furthermore, it is more cost-effective, both time-wise and financially, especially as a dictionary containing emotional aspects already exists (Bradley and Lang, 1999). The Affective Norms for English Words (ANEW) is a freely available external source, which is additionally widely used and validated by the scientific community, strengthening the credibility of the paper. This also guarantees the reproducibility of their research. Given that ANEW contains complete, instead of stemmed words, it is easy to be augmented using word embedding techniques.

There are only a few downsides of ANEW, both of which the authors seek solutions for. First, the dataset contains American English words, which needed to be changed to British English before being applied to the parliamentary debates in the UK. Second, due to the specificity of the field the authors want to apply to the dictionary, the vocabulary in the ANEW dictionary is not sufficient for the analysis of parliamentary debates. Therefore, the authors had to carry out additional steps to make ANEW applicable to the political vocabulary. To add context-specific words, as we will discuss later, they used word embedding techniques, through which they augmented the ANEW dictionary to apply for their specific research purpose.

Instead of manually adding words, the authors use word embedding techniques to expand their vocabulary in a more systematic and transparent way. ANEW further contains the standard deviation on the emotional aspect of the words, which enables the authors to categorise them as emotional or neutral with high confidence.

There are some weaknesses that the authors haven’t considered for a dictionary-based classifier, which is related to the context of the words. Even though some speeches contain similar frequencies of neutral and emotional words, the overall rhetoric could appeal to the emotion of the audience, and their score would not capture it as is based on counts. Furthermore, the context of the words could be a pitfall for the dictionary classifier because some neutral words could be used in different contexts.

<a name="1.2"></a>
## 1.2. ANEW Dictionary Augmentation

As mentioned before the ANEW dictionary is only covering a fraction of words in terms of the political context. To expand their vocabulary for their analysis, the authors use word embedding techniques. They first use their corpus of parliamentary speeches to build a skip-gram model. In parallel to that, they identify seed words extracted from the ANEW dictionary, differentiating 851 emotive and 970 neutral words. Here, they specifically focus on unambiguity regarding their emotional meaning by looking at ratings with low standard deviation in the ANEW dictionary. They furthermore adjust the spelling of the words to British English, to align them with their target context. However, we note that words might still have different connotations depending on whether they appear in an American context (such as the case for the ANEW dictionary) or in a British context (such as when they are used in the UK Parliament).

After having prepared the embedding model, their next step is to calculate the emotive score for each word in the corpus based on their seed dictionary, using cosine similarity measures (as explained by Hovy 1, p. 39-40). To calculate the emotional meaning of a word, they simply subtract the sum of the words’ cosine similarities with the set of neutral words from the sum of the words’ cosine similarities with the set of emotive words. With that, they can identify the emotional meaning of words that the parliamentary corpus contains, but the ANEW dictionary doesn’t, enabling them systematically to compute the emotional score for all parliamentary speeches. As a result, the authors expanded their vocabulary to 2.015 emotive and 2.095 neutral words, which makes their subsequent research process much more grounded in the desired context.

Sensing the critique of relying on arbitrarily defined dictionaries, OHR expanded a predefined one using word-embedding techniques, thus minimising human involvement, and therefore, human bias. The authors could conduct more reliable research by systematically augmenting the ANEW dictionary using word embedding techniques. Additionally, using a publicly available dictionary and word embedding algorithms instead of manually adding and annotating words made their research process reproducible and more credible. By using these methods, they can successfully identify the emotional meaning of speeches in the UK Parliament to observe their variance.

<a name="1.3"></a>
## 1.3. Alternative Method

An alternative way, as the authors also mention, is to run a supervised learning model to adequately categorise the speeches as “emotional” or “neutral”. Nevertheless, we would need to label them manually, which is unfeasible for the current research. Reflecting on the main assumption of the researchers, that emotive rhetoric in parliament can be measured by the words of the speech, we propose a scaling method for measuring the unobserved rhetoric of speeches by the words of the political speeches.

On one hand, Laver et al. (2003) suggest a supervised solution called “Word Scores” (Nanni et a.l, 2021). The approach would score individual words from a referenced set where the rhetoric, either neutral or emotional, is known. Based on the prevalence of words in the reference documents, the rest of the speeches are classified using the reference words. To have a strong Word Score model, it is crucial to use relevant reference texts as input, as the model would ignore the words that are not part of the reference set. Overall, the algorithm involves minimal human judgement, therefore minimising human bias with the exception of defining the reference texts, which arguably introduces human bias. It is furthermore easy to apply, given the algorithm has been shared by Laver et al. (2003). However, we found two pitfalls for this approach in the current research. First, the data is collected over a period of 19 years, and the words used in 2001 for appealing to emotion could be different than the words in 2019. That would mean making a sample of the reference set for each period of time and labelling these speeches as emotional or neutral could be unfeasible to the project budget. Second, identifying extreme cases of emotional and neutral cases could be a challenging task in this setting, involving a close reading of different texts. If on average, the speeches with higher audiences use more emotional words, it is not enough to use them as a reference set. The researcher should look into each type of speech (PMQs, Queen’s Speech debates, Ministerial Question Times, and Urgent Questions) for finding extreme cases of both neutral and emotive, which could again be impractical to the researcher’s limitations.

To overcome its limitations, the “Wordfish” unsupervised methodology was suggested by Slapin and Proksch (2008) for measuring latent variables from the texts (Nanni et al., 2021). The statistical framework of Wordfish fits the current case as the researchers rely on the relation between the frequencies of emotional and neutral words with emotional rhetoric, which in this setting is the unobserved variable. In a nutshell, the frequency of each word in each speech is modelled by the frequency of words in the speech based on how frequently the word is used in other speeches, the relationship between the word and the emotional rhetoric, and the emotional rhetoric of the speech. In contrast to the Word Scores framework, the model allows the researcher to include the variation of time when modelling the number of words, allowing them to account for the time dynamics. The approach is also able to measure emotional rhetoric for each type of speech, but in contrast to the Word Scores, the researcher does not need to look at different cases by a close reading and labelling, but by making a model for each one. Moreover, the different parameters that are estimated by the model can be used for validating it. For example, it can be checked whether the words that are most frequently used in the speeches are relevant to measuring the association between emotive rhetoric and words.

The model has three downsides to consider. First, it requires a lot of data because of the number of parameters that are being estimated. The absence of that could give the researcher unreliable estimations of the parameters and therefore, unreliable results on the scale of emotional rhetoric. Second, the model relies on the apriori distributions, and starting with the wrong distribution could lead to a model that does not converge in the estimation of parameters (Slapin & Proksch, 2008). Third, the model is more computationally expensive than the “Word Scores” and it could take a substantial amount of time to train in contrast to the flexible approach of Word Scores. Consequently, pondering the advantages and disadvantages of this particular problem, we suggest conducting the Wordfish algorithm for measuring emotive rhetoric in UK parliament debates because it covers most of the weaknesses of Word Scores by avoiding manual coding, which the authors emphasise as the main issue for using dictionary-based classifiers.

<a name="2"></a>
# Part 2: Detecting Harassment

### Intro

Classifying harassment from a set of 100.000 emails from customers could protect the support team by filtering to them only the non-harassment emails. Computational tools such as dictionary-based classifiers, topic modelling, and supervised models could help to perform this task without having a person that has to closely read each of them. However, none of them would perform perfectly either by classifying wrongly harassment (False Positives), which could impact the goodwill of the company as non-harassment emails would not arrive to the support team, or by classifying wrongly the non-harassment (False Negative) as the support team would spend time on them which could affect the overall operation.

<a name="2.1"></a>
## 2.1. Dictionary Method

The computationally cheapest of them is the dictionary method, where the emails would be classified according to a set of queries using words. Besides the (1) low computational costs, the advantages of dictionary-based classification in this scenario are (2) the “functionality of scoring texts along several dimensions, such as emotions or psychological traits” (Hovy 1, p.38). In addition, (3) the human reader can manually modify and add categories based on previous knowledge and understanding of a specific area of study, which might allow for better identification of harassment in this case.

However, this approach would not be beneficial due to several reasons:

(1) First, language is dynamic and continuously changing. Words that might identify a topic now might change over time or cultural aspects. Words that are right now related to harassment will be different within a few years, and each human reader might identify harassment differently, introducing bias to the model.

(2) In addition, language is contextual, and specific queries might not capture this relational aspect. According to Hovy, manually created dictionaries will partially overlap with the topic of interest, hence providing high precision but low recall (Hovy 1, p. 38). In the case of harassment, a human annotator will be able to bring up harassment-related keywords and these will be successfully identified (high precision), but there will be many other keywords related to harassment that will not be identified, as the human coder has a limited capacity of bringing up keywords (low recall). Indeed, it has been proven that humans are not good at bringing up keywords (King, Lam and Roberts, 2017). King, Lam and Roberts also provide a computer-assisted framework to improve keyword search by training classifiers, extracting information from their mistakes instead of correcting it, and summarising the results with Boolean search strings (2017).

<a name="2.2"></a>
## 2.2. Topic Modelling

Topic modelling overcomes the researcher bias stemming from the manual selection of keywords in the dictionary-based method. Topic modelling based on Latent Dirichlet Allocation (LDA) assumes a latent word generation process behind the distribution of words in a document, where the co-occurrence of words is related to corresponding topics. Based on the output of a topic model, we can thus determine what topic corresponds to harassment based on the association between the words related to the topic. The model requires no prior information about corporate emails but instead relies on the probabilistic distribution of words to indicate the underlying main themes (Blei, 2012). However as the method is unsupervised, two potential problems emerge.

First, there is no guarantee that harassment presents itself as a topic based on the LDA. Topic modelling is aimed at discovery, whereas this task in part presumes the presence of harassment in the data, which is not guaranteed. This highlights the conflict stemming from using an inductive method to solve a primarily deductive task. Additionally, even if harassment is present in the corpus itself, LDA may still fail in representing all topics especially if the topic sizes are unbalanced as noted by Carlsen and Ralund (2022). Thus if harassment is too uncommon in our corpus of emails, it may not necessarily appear as a discernible topic when the granularity is not high enough. We may mitigate this issue by increasing the parameter for K number of topics, but adjusting K and iteratively retraining until a topic related to harassment can be discerned raises concerns in terms of validity as it can be compared to the exploitation of p-hacking.

Second, while the topic may appear as a result of LDA, the researcher’s role and influence are not eliminated. The topics will still require a level of interpretation to determine which set of words is related to harassment. We may report on the prevalence of harassment in the corpus of 100.000 emails. However introducing new emails will require retraining the model on the updated corpus, with the problems outlined so far being given an additional chance to emerge. A possible solution is to integrate the findings of the topic modelling (under the assumption that harassment is discernible as a topic) with other models. That involves the aforementioned dictionary-based method, where the words assigned by the LDA to the topic interpreted as harassment can be fed to a dictionary. It may also involve supervised learning.

<a name="2.3"></a>
## 2.3. Supervised Learning

In supervised learning, a small percentage of emails would be manually classified as harassment by human coders according to a defined codebook. In this context, the emails would be classified by workers that develop their experience and background in the company to interpret the texts and identify cases of harassment (GRS, 2022), which is one of the main advantages of them: words are interpreted in their context. They are also flexible in representing the text input differently, using countings, relative weights in contrast to the frequency of words of other emails (TF-IDF representation), or vectors using either the co-occurrence and context from other emails of the company or external to it (word embeddings). They could present probabilities so that the company could map the likelihood of harassment instead of mapping everything binarily. The number of available models could give the company a set of options for performing the classification task.

There are some pitfalls to this approach. The amount of human involvement can have its disadvantages, as it can be extremely costly in terms of time and money for the company. Considering the amount of time that the process of labelling requires by making clear codebooks, training the coders, and evaluating the reliability of their evaluations, other priorities of the workers would be delayed in this process. In addition, the human coder can introduce biases and make mistakes due to the complexity, ambiguities, limited attention, and nuanced concepts faced during manual coding (GRS, 2022).

<a name="3"></a>
# Part 3: Supervised Text Classification

<a name="3.0"></a>
## 3.0. Packages, Data, Functions and Transformation

<a name="3.0.1."></a>

###  3.0.1. Packages
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# !pip install datasets==2.2.1 transformers==4.19.1

import warnings
warnings.filterwarnings("ignore")
from IPython.display import clear_output
import sys
import os
import datasets
import re
import string
import nltk
import pandas as pd
nltk.download('wordnet', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)
from nltk.tokenize import TweetTokenizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk import word_tokenize, pos_tag
from nltk.corpus import wordnet
from collections import defaultdict
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
import torch

from datasets import load_metric
import gensim.downloader
import numpy as np
from transformers import pipeline, Trainer, TrainingArguments
from transformers import AutoTokenizer, AutoModelForSequenceClassification

# Models
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.svm import SVC, LinearSVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, f1_score
from sklearn.metrics import confusion_matrix, recall_score, precision_score
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix

import random
torch.manual_seed(42)
random.seed(42)
np.random.seed(42)
clear_output()

"""<a name="3.0.2."></a>

### 3.0.2. Data
"""

hate_train = datasets.load_dataset('tweet_eval', 'hate', split='train')
hate_val = datasets.load_dataset('tweet_eval', 'hate', split='validation')
hate_test = datasets.load_dataset('tweet_eval', 'hate', split='test')
clear_output()

"""<a name="3.0.3."></a>

### 3.0.3. Functions
"""

def preprocess(text, mode = None):

    #Lowercasing words
    text = text.lower()

    #Removing '&amp' which was found to be common
    text = re.sub(r'&amp','', text)

    #Replace other instances of "&" with "and"
    text = re.sub(r'&','and', text)

    #Removing mentions
    text = re.sub(r'@\w+ ', '', text)

    #Removing 'RT' and 'via'
    text = re.sub(r'(^rt|^via)((?:\b\W*@\w+)+): ', '', text)

    #Removing punctuation
    #Using the module string to define what punctuation we want removed
    my_punctuation = string.punctuation.replace('#','')
    my_punctuation = my_punctuation.replace('-','')
    my_punctuation = my_punctuation+'’“”—'
    # Text is "translated" to not include the punctuation we defined
    text = text.translate(str.maketrans('', '', my_punctuation))

    #Removing odd special characters
    #removing dash lines bounded by whitespace (and therefore not part of a word)
    text = re.sub(r' - ','', text)
    text = re.sub(r"[┻┃━┳┓┏┛┗]","", text)
    text = re.sub(r"\u202F|\u2069|\u200d|\u2066","", text)

    #Removing URLs
    text = re.sub(r'http\S+', '', text)

    #Removing numbers
    text = re.sub(r'[0-9]','', text)

    #Tokenizing (separators and superfluous whitespace are also removed during tokenization)
    tokenizer = TweetTokenizer()
    text = tokenizer.tokenize(text)

    if mode == "lem":
      lemmatizer = WordNetLemmatizer()
      tag_map = defaultdict(lambda : wordnet.NOUN)  #If nothing else is specified, use noun tag
      tag_map['J'] = wordnet.ADJ
      tag_map['V'] = wordnet.VERB
      tag_map['R'] = wordnet.ADV
      #Lemmatizing each word in the sentence with list comprehension
      text_lemmatized = [lemmatizer.lemmatize(word, tag_map[tag[0]]) for
                         word, tag in pos_tag(text)]
      #Notice above that we choose tag[0] to get all instances of a word class,
      #e.g. NN (noun) and NP (proper noun) should both translate to noun.

      text = ' '.join(text_lemmatized)

    return text

"""<a name="3.0.4."></a>

### 3.0.4. Transform Data
"""

# create corpus objects
for part in ["train","val","test"]:
  globals()[part+"_corpus"] = [x['text'] for x in globals()["hate_"+part]]
  globals()[part+"_corpus_lem"] = [preprocess(x, "lem") for x in globals()[part+"_corpus"]]
  globals()[part+"_labels"] = [x['label'] for x in globals()["hate_"+part]]

# vectorizing the corpuses
vectorizer = TfidfVectorizer()
for part in ["train","val","test"]:
  if part == "train":
    globals()[part+"_features"] = vectorizer.fit_transform(globals()[part+"_corpus"])
  else:
    globals()[part+"_features"] = vectorizer.transform(globals()[part+"_corpus"])

feature_names = vectorizer.get_feature_names_out()

# vectorizing the lemmatized corpuses
vectorizer = TfidfVectorizer()
for part in ["train","val","test"]:
  if part == "train":
    globals()[part+"_features_lem"] = vectorizer.fit_transform(globals()[part+"_corpus_lem"])
  else:
    globals()[part+"_features_lem"] = vectorizer.transform(globals()[part+"_corpus_lem"])

feature_names_lem = vectorizer.get_feature_names_out()

"""<a name="3.1"></a>
## 3.1. BoW Models

<a name="3.1.1."></a>

### 3.1.1. Logistic Regression on Unprocessed Data
"""

param_grid = {'C': [ 1, 9, 9.1, 9.2, 9.3, 9.4, 9.5, 9.6,
                    9.7, 9.8, 10, 10.5, 11]} # best param = 9
lr_grid = GridSearchCV(LogisticRegression(penalty="l2", max_iter=1000),
                       param_grid, cv=5, n_jobs=-1)
lr_grid.fit(train_features, train_labels)
lr_test_preds = lr_grid.best_estimator_.predict(test_features)

results = {"Accuracy":dict(),"F1 score":dict(),"Precision":dict(), "Recall":dict()}
results["Accuracy"]["LR"] = accuracy_score(test_labels, lr_test_preds)
results["F1 score"]["LR"] = f1_score(test_labels, lr_test_preds, average="macro")

"""<a name="3.1.2."></a>

### 3.1.2. Logistic Regression on Preprocessed Data
"""

param_grid = {'C': [0.001, 0.01, 0.1, 0.5, 0.8, 1, 1.01,
                    1.02, 1.03, 1.1, 1.2, 1.5, 2, 2.001, 2.002, 2.1]} # best param = 2.002
lr_grid = GridSearchCV(LogisticRegression(penalty="l2"), param_grid,
                       cv=5, n_jobs=-1, scoring='precision')
lr_grid.fit(train_features_lem, train_labels)
lr_test_preds_lem = lr_grid.best_estimator_.predict(test_features_lem)

results["Accuracy"]["LR_lem"] = accuracy_score(test_labels, lr_test_preds_lem)
results["F1 score"]["LR_lem"] = f1_score(test_labels, lr_test_preds_lem, average="macro")

"""<a name="3.1.3."></a>
### 3.1.3. Linear SVC on Unprocessed Data
"""

linearSVC = LinearSVC()
linearSVC_score = cross_val_score(LinearSVC(), train_features, train_labels,
                                  cv=StratifiedKFold(n_splits=5), scoring="accuracy", n_jobs=-1)
param_grid = {'C': [0.001, 0.01, 0.1, 0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7,
                    0.8, 0.9, 1, 1.001, 1.002, 1.003, 1.1, 10]} # best param = 0.5
linearSVC_grid = GridSearchCV(LinearSVC(), param_grid, cv=5, n_jobs=-1, scoring='precision')
linearSVC_grid.fit(train_features, train_labels)

linearSVC_test_preds = linearSVC_grid.best_estimator_.predict(test_features)
results["Accuracy"]["SVC"] = accuracy_score(test_labels, linearSVC_test_preds)
results["F1 score"]["SVC"] = f1_score(test_labels, linearSVC_test_preds, average="macro")

"""<a name="3.1.4."></a>
### 3.1.4. Linear SVC on Preprocessed Data
"""

linearSVC_score_lem = cross_val_score(LinearSVC(), train_features_lem, train_labels,
                                      cv=StratifiedKFold(n_splits=5),
                                      scoring="accuracy", n_jobs=-1)
param_grid = {'C': [0.001, 0.01, 0.02, 0.03, 0.05, 0.07, 0.09, 0.1,
                    0.11, 0.12, 0.15, 0.2, 0.3, 1, 10]} # best param = 0.3
linearSVC_grid = GridSearchCV(LinearSVC(), param_grid, cv=5, n_jobs=-1,
                              scoring='precision')
linearSVC_grid.fit(train_features_lem, train_labels)

linearSVC_test_preds_lem = linearSVC_grid.best_estimator_.predict(test_features_lem)
results["Accuracy"]["SVC_lem"] = accuracy_score(test_labels, linearSVC_test_preds_lem)
results["F1 score"]["SVC_lem"] = f1_score(test_labels, linearSVC_test_preds_lem, average="macro")

"""<a name="3.1.5."></a>
### 3.1.5. Multinomial Naive Bayes on Unprocessed Data
"""

count_vectorizer = CountVectorizer()
train_features_count = count_vectorizer.fit_transform(train_corpus)
val_features_count = count_vectorizer.transform(val_corpus)
test_features_count = count_vectorizer.transform(test_corpus)

nb = MultinomialNB()
nb_score = cross_val_score(nb, train_features_count.toarray(), train_labels,
                           cv=StratifiedKFold(n_splits=5), scoring="precision", verbose=2)
nb.fit(train_features_count.toarray(), train_labels)

nb_test_preds = nb.predict(test_features_count.toarray())
results["Accuracy"]["NB"] = accuracy_score(test_labels, nb_test_preds)
results["F1 score"]["NB"] = f1_score(test_labels, nb_test_preds, average="macro")
clear_output()

"""<a name="3.1.6."></a>
### 3.1.6. Multinomial Naive Bayes on Preprocessed Data
"""

train_features_count_lem = count_vectorizer.fit_transform(train_corpus_lem)
val_features_count_lem = count_vectorizer.transform(val_corpus_lem)
test_features_count_lem = count_vectorizer.transform(test_corpus_lem)

nb = MultinomialNB()
nb_score_lem = cross_val_score(nb, train_features_count_lem.toarray(),
                               train_labels, cv=StratifiedKFold(n_splits=5),
                               scoring="precision", verbose=2)
nb.fit(train_features_count_lem.toarray(), train_labels)

nb_test_preds_lem = nb.predict(test_features_count_lem.toarray())
results["Accuracy"]["NB_lem"] = accuracy_score(test_labels, nb_test_preds_lem)
results["F1 score"]["NB_lem"] = f1_score(test_labels, nb_test_preds_lem, average="macro")
clear_output()

"""<a name="3.2"></a>
## 3.2. Deep Learning

<a name="3.2.1."></a>

### 3.2.1 RNN-LSTM Models
"""

# defining the vocabulary based on tokens in the train and val sets
total_vocabulary = set()
for tweet in train_corpus + val_corpus:
    tokens = TweetTokenizer().tokenize(tweet)
    for t in tokens:
        total_vocabulary.add(t.lower())
total_vocabulary = sorted(list(total_vocabulary))
total_vocabulary = [""]+total_vocabulary


def create_embedding_matrix(tokens, embedding):
    """creates an embedding matrix from pre-trained embeddings for a new vocabulary.
    It also adds an extra vector
    vector of zeroes in row 0 to embed the padding token, and initializes
    missing tokens as vectors of 0s"""
    oov = set()
    size = embedding.vector_size

    embedding_matrix=np.zeros((len(tokens),size))
    c = 0
    for i in range(1,len(tokens)):
        try:
            embedding_matrix[i]=embedding[tokens[i]]
        except KeyError: #to catch the words missing in the embeddings
            try:
                embedding_matrix[i]=embedding[tokens[i].lower()]
            except KeyError:
                #if the token does not have an embedding, we initialize it as a vector of 0s
                embedding_matrix[i] = np.zeros(size)
                #we keep track of the out of vocabulary tokens
                oov.add(tokens[i])
                c +=1
    print(f'{c/len(tokens)*100} % of tokens are out of vocabulary')
    return embedding_matrix, oov

# load the pretrained embeddings for twitter data
glove = gensim.downloader.load('glove-twitter-200')

#get the embedding matrix for our tweet_eval vocabulary
embedding_matrix, oov = create_embedding_matrix(total_vocabulary, glove)

def text_to_indices(text, total_vocabulary):
    """turns the input text (one tweet) into a vector of indices in
    total_vocabulary that corresponds to the tokenized words in the input text"""
    encoded_text = []
    tokens = word_tokenize(text)
    for t in tokens:
        try:
            index = total_vocabulary.index(t.lower())
            encoded_text.append(index)
        except:
            continue
    return encoded_text

def add_padding(vector, max_length, padding_index):
    """adds copies of the padding token to make the input vector
    the max_length size, so that all inputs are the same length
    (the length of tweet with most words)"""
    if len(vector) < max_length:
        vector = [padding_index for _ in range(max_length-len(vector))] + vector
    return vector

class TweetEvalTrain(torch.utils.data.Dataset):
    # defining the sources of the data
    def __init__(self, features, labels):
        self.X = torch.LongTensor(features)
        self.y = torch.from_numpy(np.array(labels)).type(torch.LongTensor)

    def __getitem__(self, index):
        X = self.X[index]
        y = self.y[index].unsqueeze(0)
        return X, y

    def __len__(self):
        return len(self.y)

# converting the tokens to indices (so its machine readable)
for part in ["train","val","test"]:
  globals()[part+"_features"] = [text_to_indices(x, total_vocabulary)
  for x in globals()[part+"_corpus"]]

# padding the feature vectors in each split
longest_tweet = max(train_features+val_features, key=len)
max_length = len(longest_tweet)
padding_index = 0

for part in ["train","val","test"]:
  globals()[part+"_features"] = [add_padding(x, max_length, padding_index)
  for x in globals()[part+"_features"]]

# setting up data loaders
data_train = TweetEvalTrain(train_features, train_labels)
data_val = TweetEvalTrain(val_features, val_labels)
data_test = TweetEvalTrain(test_features, test_labels)

for part in ["train","val","test"]:
  globals()[part+"_loader"] = torch.utils.data.DataLoader(globals()["data_"+part],
                                                          batch_size = 64)

def training_loop(model, num_epochs):
    loss_function = torch.nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

    for epoch in range(num_epochs):
        losses = []
        for batch_index, (inputs, targets) in enumerate(train_loader):

            optimizer.zero_grad()
            outputs = model(inputs).squeeze()
            targets = targets.squeeze()
            loss = loss_function(outputs, targets)
            loss.backward()
            optimizer.step()
            losses.append(loss.item())

    return model

def evaluate(model, val_loader):
    predictions = []
    labels = []
    with torch.no_grad():
        for batch_index, (inputs, targets) in enumerate(val_loader):
            outputs = torch.softmax(model(inputs), 1 )
            vals, indices = torch.max(outputs, 1)
            predictions += indices.tolist()
            labels += targets.tolist()

    acc = accuracy_score(predictions, labels)
    print(f'Model accuracy: {acc}')
    return acc, predictions

# setting up the model object, whose architecture can be configured for either RNN or LSTM
class RNN_or_LSTM(torch.nn.Module):
    def __init__(self, rnn_size, n_classes, embedding_matrix, arch="RNN"):
        # initialize the model with a certain dimension of the RNN unit activations (this is rnn_size)
        # and a certain number of output classes

        super().__init__()
        self.arch = arch # specify model type

        #applying the embeddings to the inputs
        self.embedding = torch.nn.Embedding.from_pretrained(
            torch.FloatTensor(embedding_matrix), padding_idx=0, freeze=True)
        emb_dim = embedding_matrix.shape[1]

        if self.arch == "RNN":
            self.rnn = torch.nn.RNN(input_size=emb_dim, hidden_size=rnn_size,
                                    num_layers=1, batch_first=True)
        else: #assumes self.arch == "LSTM":
            self.rnn = torch.nn.LSTM(input_size=emb_dim, hidden_size=rnn_size,
                                     bidirectional=False, num_layers=1, batch_first=True)
        self.fc_logits = torch.nn.Linear(rnn_size, n_classes)

    def forward(self, inputs):

        # encode the input vectors
        encoded_inputs = self.embedding(inputs)

        #apply the RNN or LSTM
        if self.arch == "RNN":
            rnn_out, final_state = self.rnn(encoded_inputs)
        else:
            # slight difference in output of LSTM resulting in different assignment
            rnn_out, (final_state, c_n) = self.rnn(encoded_inputs)

        # run the final states through the output layer
        outputs = self.fc_logits(final_state.squeeze())
        return outputs

# initializing the models and training them for 10 epochs
RNN = RNN_or_LSTM(rnn_size=100, n_classes=2, embedding_matrix=embedding_matrix, arch = "RNN")
LSTM = RNN_or_LSTM(rnn_size=100, n_classes=2, embedding_matrix=embedding_matrix, arch ="LSTM")

RNN = training_loop(RNN, 10)
rnn_acc, rnn_pred = evaluate(RNN, test_loader)

LSTM = training_loop(LSTM, 10)

lstm_acc, lstm_pred = evaluate(LSTM, test_loader)

# initializing the models and training them for 10 epochs
RNN = RNN_or_LSTM(rnn_size=100, n_classes=2, embedding_matrix=embedding_matrix, arch = "RNN")
LSTM = RNN_or_LSTM(rnn_size=100, n_classes=2, embedding_matrix=embedding_matrix, arch ="LSTM")

RNN = training_loop(RNN, 10)
LSTM = training_loop(LSTM, 10)

for model in ["RNN", "LSTM"]:
  acc, pred = evaluate(globals()[model], test_loader)
  results["Accuracy"][model] = acc
  results["F1 score"][model] = f1_score(test_labels, pred, average="macro")

"""<a name="3.2.2."></a>

### 3.2.2 BERT fine-tuned
"""

# library

# gpu name
device = "cuda:0" if torch.cuda.is_available() else "cpu"
# name of the pretrained model
bert_name = 'prajjwal1/bert-medium'
# bert tokenizer is special (wordpiece)
bert_tokenizer = AutoTokenizer.from_pretrained(bert_name)
# function for tokenize each text
def bert_tokenize(dataset):
  return bert_tokenizer(dataset['text'])
# apply bert tokenizer to the texts
bert_tokenized_train_dataset = hate_train.map(bert_tokenize,batched=True)
bert_tokenized_val_dataset =  hate_val.map(bert_tokenize,batched=True)
# get the pre-train model to fine-tunning with two possible outputs
bert_model_fine_tune = AutoModelForSequenceClassification.from_pretrained(bert_name, num_labels=2)
# move to GPU
bert_model_fine_tune.to(device)
# path to save the arguments
output_dir_ = "path/to/output/directory"
# define the arguments of the model
training_args = TrainingArguments(num_train_epochs=3,
                                  per_device_train_batch_size=8,
                                  output_dir=output_dir_,
                                  logging_steps=500)
# evaluation metrics model
metric_f1 = load_metric("f1")
metric_acc = load_metric("accuracy")
def compute_metrics(eval_pred):
  outputs, labels = eval_pred
  predictions = np.argmax(outputs, axis=-1)
  f1 = metric_f1.compute(predictions=predictions, references=labels)
  acc = metric_acc.compute(predictions=predictions, references=labels)
  return f1 | acc

# define trainer to train the model
trainer = Trainer(model=bert_model_fine_tune,
                  args=training_args,
                  compute_metrics=compute_metrics,
                  train_dataset=bert_tokenized_train_dataset,
                  eval_dataset=bert_tokenized_val_dataset,
                  tokenizer=bert_tokenizer)
# train the model
trainer.train()

# Run evaluation
evaluation_results = trainer.evaluate()

# Get accuracy from the evaluation results
accuracy = evaluation_results["eval_accuracy"]


clear_output()

bert_tokenized_test_dataset = hate_test.map(bert_tokenize,batched=True)
clear_output()

predictions = trainer.predict(bert_tokenized_test_dataset)
predicted_labels = predictions.predictions
clear_output()

results["Accuracy"]["BERT"] = accuracy_score(np.argmax(predicted_labels,axis=1),
                                             bert_tokenized_test_dataset['label'])
results["F1 score"]["BERT"] = f1_score(np.argmax(predicted_labels,axis=1),
                                       bert_tokenized_test_dataset['label'], average="macro")

"""<a name="3.3"></a>
## 3.3. Models Discussion

<a name="3.3.1."></a>

### 3.3.1. BoW

<a name="3.3.1.1"></a>

#### 3.3.1.1. Logistic Regresion

Logistic regression is the most common ML classifier (Hovy 2, p. 11). By running a logistic regression model, we can easily predict binary outcomes such as hate speech (HS) and non-HS texts from a set of features. What is more, it doesn’t use much computational power, making the prediction process fast. It is more useful than running a linear model in the sense that 1) its predictions are values between 0 and 1 which can be interpreted in a probabilistic context and 2) outliers are not overweighted. The downside of the logistic regression model is that it misses non-linear associations by representing the relation between features and the outcome using only one sigmoid function

<a name="3.3.1.2."></a>

#### 3.3.1.2. Support Vector Classifier (SVC):

The SVC approach is computationally slower than logistic regression but much more reliable for classifying outputs with larger datasets. Even though this approach has a high time complexity when dealing with very large dataframes, this is not an issue with the size of our dataframe.

On the other hand, the outputs are only predictions and not probabilities, so it is not as helpful as Naive Bayes or Logistic Regression models when it comes to flexibility, as probability approaches allow for changing thresholds, hence penalising false positives in our particular case. Both the accuracy and the F1 scores are the highest when using SVC in comparison to Logistic Regression and Multinomial Naive Bayes, but it is important to note that this model predicts a high amount of false negatives, which can be detrimental to the final task: identifying HS within tweets.

<a name="3.3.1.3."></a>

#### 3.3.1.3. Multinomial Naive Bayes:

This particular model is faster and simpler than SVC, and it performs well in small datasets, as it is in this case, considering that the full corpus we are working with contains 12.970 instances. In addition, the model outputs class probabilities, which is helpful for interpreting and analysing the outputs in relation to specific tweets, and for adapting the threshold to penalise false positives. Unfortunately, and as it happens with logistic regression models, Naive Bayes misses important interactions, hence predicting a high number of false positives (1.501 in the case of unprocessed text and 1.494 in the case of preprocessed text, the highest of these three models).

<a name="3.3.2."></a>

### 3.3.2. Deep Learning (RNN-LSTM)

In contrast to the bag of words approach often found in TF-IDF-based models (see above), Recurrent Neural Networks (RNNs) are able to take the entire sequence of tokens in a tweet into account. That means that an RNN model is able to capture the potential nuances stemming from the order in which HS-related tokens appear. For example, tweets defined by having slurs placed at the end or beginning of a tweet may carry a different meaning than those with those same slurs in the middle of the sentence. Long Short-Term Memory (LSTM) models are similar to RNN in this regard. However, LSTMs are in theory also able to learn what tokens introduce noise and filter them out accordingly while retaining relevant information.

In both models, pre-trained word embeddings are employed to enrich the models’ associated understanding of the tokens. In this case, we specifically use word embeddings trained on Twitter data, which in theory should better match the vectors of the word embeddings in this case.

<a name="3.3.3."></a>

### 3.3.3. Deep Learning (BERT)

Fine-tuning the BERT model for predicting HS makes sense as the words are understood in a broader context. In this approach, the key is that the network learns the meaning of the whole sentence by looking at the relevant parts to consider while processing. The BoW representation is changed as the model takes sequences of words as inputs. However, the model does not process the words sequentially, as RNN or LSTM does, which is computationally costly as the backward-forward propagation is performed many times in the mentioned models, but it takes into account the order of the words by positional encoding (Jurafsky, 2023).

In this context, however, the pre-trained model architecture is as important as the train data of which it was developed. The BERT model was initially trained on Wikipedia and a corpus with American English texts. Even with millions of texts, in general, the semantics and words from the sampled tweets could behave differently and do not represent the same vocabulary and semantics as the training dataset. For example, British English and American English differ significantly in word usage and meaning, especially when it comes to slurs. As we are working with tweets that inherently contain lots of slang and slurs, the model might fail to capture such distinctions present among both British and American English.

<a name="3.4"></a>
## 3.4. Performance
"""

import pandas as pd
pd.DataFrame(results)[['Accuracy', 'F1 score']].sort_values('F1 score',
                                                            ascending=False).round(3)

"""The highest performance model based on accuracy is SVC with lemmatized iput. Looking at the F1 metric, the best model is SVC.
Accuracy measures the number of correct predictions based on all of the predictions. The F1 metric takes both precision and recall scores into account. In our models, the precision scores are relatively low in comparison to our recall scores, meaning that the models barely miss any true positives, but we cannot really trust the positive predictions provided by them, as there is a high number of false positives. As a result, we argue that the F1 score can better capture the nuances of this dichotomy between precision and recall better than the accuracy metric.

We conclude that, despite using BoW or word embeddings as input, all of the models perform poorly.

<a name="3.5"></a>
## 3.5. Discussion

<a name="3.5.1."></a>
#### 3.5.1 Data Collection

The authors developed three approaches for sampling their tweets: user-based (potential victims and identified haters), and keywords-based. They also modified the distribution of the labels, by overrepresenting the HS texts from the original dataset where only 10% contains HS. We would like to have a clear description of the mentioned process for two reasons in order to understand the performance of the models. First, it is unclear why they used three approaches instead of one, and the dispersion of texts between samples. Knowing these details would give us an idea of how different each dataset represents HS by text. Even though HS is labelled with the same codebook, the way that HS is represented in each sample could be different, and controlling the variation from each sample could improve the accuracy of the model.

Oversampling HS makes sense because ML models with  unbalanced dataset tend to have low precision by predicting  from the majority of cases (in this case non-HS). Nevertheless, the consequences of classifying using the new data set are as bad as the beginning: low precision where the majority is predicted as HS. Having an idea on (1) how the modified dataset was constructed, (2) the content of the text overrepresented, and (3) how to obtain the original dataset; could give us insights to improve the models.

<a name="3.5.1."></a>
#### 3.5.1 Data Collection

Another aspect that could provide useful information is further insight into the annotation process. It is unclear how cases of HS that are related to neither women nor immigrants are labelled. This raises the question of whether such cases are sufficiently distinct from the two forms of HS of interest such that non-women and non-immigrant-targeting HS does not obscure the words related to positive cases. By extension, in terms of HS targeted towards women, it is not clear how gender is defined. It is unclear whether this includes non-cisgender women. And if so, whether HS targeted towards non-cisgender women is expressed in a way that is comparable to HS targeted towards cisgender women. The annotation process included crowd-sourced in addition to two expert annotations with labelling based on the majority of these three sources. In terms of this majority, two aspects may be useful in terms of taking ambiguity and coder disagreement into account.

First, in terms of the crowdsourced data, it may provide insight to know the level of inter-rater agreement for each tweet, as they only report the overall level of inter-rater agreement (83% for English dataset, 89% for Spanish dataset). This would possibly aid in determining how large a subset of wrongfully predicted data is due to a potential deficiency in part of our models or is due to ambiguity in the data itself (where the annotation might be misleading and/or context-dependent). Second, the level of ambiguity may also be taken into account by noting whether the agreement between the crowd-sourced annotation and the two expert annotations is based on full consensus or just a majority. And if a final label is not based on a full consensus, being able to note the source of disagreement may be especially relevant if we are able to take the coder's background into account, such as whether the coder identifies as a woman or has an immigrant background.

<a name="4"></a>
# Part 4: Word Embeddings

<a name="4.0"></a>
## 4.0. Packages
"""

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')
path = '/content/drive/MyDrive/ASDS II./Exam'
os.chdir(path)

"""<a name="4.1"></a>
## 4.1. Training

Train your own word2vec vectors on the dataset. We recommend a vector size of 200, a window of 5, and three epochs of training. Make sure you tokenize and lowercase the words before feeding them into the model.
"""

# code from ASDS2 Exercise 5.1 solutions:
class MyDataLoader(object):
    """
    A DataLoader class for reading and iterating over a corpus file.

    Args:
        filename (str): The name of the corpus file.
    """
    # initialize the corpus object for a given filename
    def __init__(self, filename):
        self.corpus = filename

    # we will need to define what counts as a "chunk" in this file, so when the
    # Dataloader is loading (iterating over) the file and feeding it to the embedding
    # model, it knows what to treat as one unit. Here, we (arbitrarily) say that one
    # line in the file (corresponding to a paragraph) is one chunk.

    def __iter__(self):
    # _iter_ator function to iterate over the lines of the corpus file.
        for line in open(self.corpus, "r", encoding="utf-8"):
            # checking that the line is not empty:
            if line.strip():
            # you may do some pre-processing on-the-fly. here we tokenize and lowercase
            # the string before yielding it
                line = word_tokenize(line)
                line = [x.lower() for x in line]
                yield line

# loading UNGD speeches
UNGD_speeches = MyDataLoader("allspeeches_77_2022.txt")

# we used the skipgram model for training to be more accurate
# in the case of low frequency words (Hovy 1, p. 43)
UNGD2vec = gensim.models.Word2Vec(UNGD_speeches, vector_size=200, window=5,
                                  min_count=5, epochs=3, seed=42, sg=1)

"""<a name="4.2"></a>
## 4.2. Similarity
"""

# printing out the 10 words most similar to ["climate", "pandemic", "terrorism", "future"]
words_of_interest = ["climate", "pandemic", "terrorism", "future",
                     "biodiversity", "drug", "generations"]
msws = []

for woi in words_of_interest:
  df_temp = pd.DataFrame(UNGD2vec.wv.most_similar(woi, 10),
                         columns=[woi+"_word", woi+"_similarity"])
  msws.append(df_temp)

df_msws = pd.concat([pd.DataFrame(x) for x in msws], axis=1)
df_msws

"""In the context of word2vec, "similarity" means the semantic relation of the words. This is, in our word2vec algorithm, based on to what extent the given words are co-occuring within a given window of words (which has a size of 5) (Hovy 1, p. 42).
- **Climate:** "climate change" is often used as a compound word, while "paris" might refer to the Paris Agreement (COP21, 2015), where global leaders agreed to aim to limit the temperature increase. We suspect loss is often used as part of the compound word "biodiversity loss". Looking at the most similar words to biodiversity, adaptation and loss are indeed at the top, both of which appear in the list of "climate".
- **Pandemic:** top words are covid-19 and coronavirus, both of which are referring to the recent pandemic. Having "devastating", "effects", "consequences", "increased" and "impacts" on the list suggests an ongoing mentioning of its aftermath, ~2.5 years after its breakout.
- **Terrorism:** the most similar word is "armed", probably referring to its agressive nature. It is surprising to us that "extremism" is not on the top, nor "violence", which might sound like the most common words to describe "terrorism". We observe UNGA speeches rather co-use the word with concrete acts of (e.g. "drug") and counteracts to ("sanctions") terrorism. Taking a glance at the most frequently co-occuring words with "drug", we see that "trafficking" is on the top, indicating its appearance as a compound word in relation to "terrorism" as well.
- **Future:** "generations" shows the strongest similarity to "future", which might refer to the compund word "future generations". When looking at "generations" we find that "future" is the second on its list, indicating a similarity value close to the other way round.

<a name="4.3"></a>
## 4.3. Supervised Task

Pre-trained models generally contain word embeddings that apply to the greater society. As Thadajarassiri et al. underline, although embeddings trained on a large corpus "are broadly useful across many tasks in many domains”, their general nature “comes at the cost of ignoring word meanings that are specific to domains" (2019). In fact, Diaz et al. argue that "for many tasks that require topic-specific linguistic analysis, topic-specific representations should outperform global representations" (2016).

If we had a small dataset, it might be more useful to rely on pre-trained models (like GloVe, or Fasttext). We argue, however, that if we are curious about uncovering more specific similarities, it rather makes sense to use a locally trained model to solve our issue. Although computationally expensive and restricted in size, creating our own word embeddings is more accurate when learning meanings within the domain specific to international affairs (Hovy 1, p.45). Given the data we have includes all 77 sessions of UN General Debates and inherently contains the vocabulary of international affairs, we argue it makes more sense to use our locally trained model for the detection.

<a name="4.4"></a>
## 4.4. Re-training
"""

# training the model with a different seed
UNGD2vec_2 = gensim.models.Word2Vec(UNGD_speeches, vector_size=200, window=5,
                                    min_count=5, epochs=3, seed=42, sg=1)

# printing out the words most similar to ["climate", "pandemic", "terrorism", "future"]
msws_2 = []

for woi in words_of_interest:
  df_temp = pd.DataFrame(UNGD2vec_2.wv.most_similar(woi, 10), columns=[woi+"_word", woi+"_similarity"])
  msws_2.append(df_temp)

df_msws_2 = pd.concat([pd.DataFrame(x) for x in msws_2], axis=1)
df_msws_2

"""It seems the word embeddings aren't stable. For example, "change" is now the second most similar word with "climate", "violence" became the most similar with "terrorism", and "build" is the most frequent with "future", to show some concrete examples. This is due to the sensitivity of embedding algorithms to sandom seeds (Antoniak and Mimmo, 2018). To eliminate that issue, Antoniak and Mimmo suggest using multiple bootstrap samples and average out the results.

<a name="4.3"></a>
## 4.5. Validation
"""

UNGD2vec.wv.doesnt_match(["covid-19", "pandemic", "disease", "vaccine", "environment"])

UNGD2vec.wv.doesnt_match(["peace", "forces", "aggression", "army", "russia"])

UNGD2vec.wv.doesnt_match(["botswana", "namibia", "rwanda", "germany", "tanzania"])

UNGD2vec.wv.doesnt_match(["botswana", "namibia", "rwanda", "africa", "tanzania"])

"""It is generally very tricky to validate word embeddings. There are two categories of evaluation we can apply (GRS, Chapter 8, p. 87). We have the option to conduct extrinsic evaluation, in which we can run a machine lerning model with predefined correct answers (Rodriguez and Spirling, 2021, p.10). That would take some time. The other way to evaluate our model is to inspect how well it represents the relationships between words (intrinsic evaluation). For example, we can assess word similarity or perform clustering tasks (GRS, ibid.). Looking at our example above, we see that our model fails to tell Germany apart from countries located in Africa, which implies it failed to understand geographic associations. If we had more time and resources, we would definitely consider conducting a Turing style test (as explained by Rodriguez and Spirling, 2021), in which they assess the performance of the model with human judgement. Due to that, such an evaluation is timely and financially costly, which is why we refrain from it for now.

# References (in addition to the syllabus)
- Carlsen, H. B., & Ralund, S. (2022). _Computational grounded theory revisited: From computer-led to computer-assisted text analysis._ Big Data & Society, 9(1), 205395172210801. https://doi.org/10.1177/20539517221080146
- Diaz, F., Mitra, B., & Craswell, N. (2016). _Query Expansion with Locally-Trained Word Embeddings_ (arXiv:1605.07891). arXiv. http://arxiv.org/abs/1605.07891
- Nanni, F., Glavas, G., Rehbein, I., Ponzetto, S. P., & Stuckenschmidt, H. (2021). _Political Text Scaling Meets Computational Semantics_ (arXiv:1904.06217). arXiv. http://arxiv.org/abs/1904.06217
- Thadajarassiri, J., Sen, C., Hartvigsen, T., Kong, X., & Rundensteiner, E. (2019). _Comparing General and Locally-Learned Word Embeddings for Clinical Text Mining._ 2019 IEEE EMBS International Conference on Biomedical & Health Informatics (BHI), 1–4. https://doi.org/10.1109/BHI.2019.8834672
"""